{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f64380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test script for single agent problems.\n",
    "\n",
    "This scripts runs the best model found by one of the executions of `singleagent.py`\n",
    "\n",
    "Example\n",
    "-------\n",
    "To run the script, type in a terminal:\n",
    "\n",
    "    $ python test_singleagent.py --exp ./results/save-<env>-<algo>-<obs>-<act>-<time_date>\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy as a2cppoMlpPolicy\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy as a2cppoCnnPolicy\n",
    "from stable_baselines3.sac.policies import SACPolicy as sacMlpPolicy\n",
    "from stable_baselines3.sac import CnnPolicy as sacCnnPolicy\n",
    "from stable_baselines3.td3 import MlpPolicy as td3ddpgMlpPolicy\n",
    "from stable_baselines3.td3 import CnnPolicy as td3ddpgCnnPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_pybullet_drones.utils.utils import sync\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.FlyThruGateAviary import FlyThruGateAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.TuneAviary import TuneAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import ActionType, ObservationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shared_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #### Define and parse (optional) arguments for the script ##\n",
    "    parser = argparse.ArgumentParser(description='Single agent reinforcement learning example script using TakeoffAviary')\n",
    "    parser.add_argument('--exp',                           type=str,            help='The experiment folder written as ./results/save-<env>-<algo>-<obs>-<act>-<time_date>', metavar='')\n",
    "    ARGS = parser.parse_args()\n",
    "\n",
    "    #### Load the model from file ##############################\n",
    "    algo = ARGS.exp.split(\"-\")[2]\n",
    "\n",
    "    if os.path.isfile(ARGS.exp+'/success_model.zip'):\n",
    "        path = ARGS.exp+'/success_model.zip'\n",
    "    elif os.path.isfile(ARGS.exp+'/best_model.zip'):\n",
    "        path = ARGS.exp+'/best_model.zip'\n",
    "    else:\n",
    "        print(\"[ERROR]: no model under the specified path\", ARGS.exp)\n",
    "    if algo == 'a2c':\n",
    "        model = A2C.load(path)\n",
    "    if algo == 'ppo':\n",
    "        model = PPO.load(path)\n",
    "    if algo == 'sac':\n",
    "        model = SAC.load(path)\n",
    "    if algo == 'td3':\n",
    "        model = TD3.load(path)\n",
    "    if algo == 'ddpg':\n",
    "        model = DDPG.load(path)\n",
    "\n",
    "    #### Parameters to recreate the environment ################\n",
    "    env_name = ARGS.exp.split(\"-\")[1]+\"-aviary-v0\"\n",
    "    OBS = ObservationType.KIN if ARGS.exp.split(\"-\")[3] == 'kin' else ObservationType.RGB\n",
    "    if ARGS.exp.split(\"-\")[4] == 'rpm':\n",
    "        ACT = ActionType.RPM\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'dyn':\n",
    "        ACT = ActionType.DYN\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'pid':\n",
    "        ACT = ActionType.PID\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'vel':\n",
    "        ACT = ActionType.VEL\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'tun':\n",
    "        ACT = ActionType.TUN\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'one_d_rpm':\n",
    "        ACT = ActionType.ONE_D_RPM\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'one_d_dyn':\n",
    "        ACT = ActionType.ONE_D_DYN\n",
    "    elif ARGS.exp.split(\"-\")[4] == 'one_d_pid':\n",
    "        ACT = ActionType.ONE_D_PID\n",
    "\n",
    "    #### Evaluate the model ####################################\n",
    "    eval_env = gym.make(env_name,\n",
    "                        aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS,\n",
    "                        obs=OBS,\n",
    "                        act=ACT\n",
    "                        )\n",
    "    mean_reward, std_reward = evaluate_policy(model,\n",
    "                                              eval_env,\n",
    "                                              n_eval_episodes=10\n",
    "                                              )\n",
    "    print(\"\\n\\n\\nMean reward \", mean_reward, \" +- \", std_reward, \"\\n\\n\")\n",
    "\n",
    "    #### Show, record a video, and log the model's performance #\n",
    "    test_env = gym.make(env_name,\n",
    "                        gui=True,\n",
    "                        record=False,\n",
    "                        aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS,\n",
    "                        obs=OBS,\n",
    "                        act=ACT\n",
    "                        )\n",
    "    logger = Logger(logging_freq_hz=int(test_env.SIM_FREQ/test_env.AGGR_PHY_STEPS),\n",
    "                    num_drones=1\n",
    "                    )\n",
    "    obs = test_env.reset()\n",
    "    start = time.time()\n",
    "    for i in range(10*int(test_env.SIM_FREQ/test_env.AGGR_PHY_STEPS)): # Up to 6''\n",
    "        action, _states = model.predict(obs,\n",
    "                                        deterministic=True # OPTIONAL 'deterministic=False'\n",
    "                                        )\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        test_env.render()\n",
    "        if OBS==ObservationType.KIN:\n",
    "            logger.log(drone=0,\n",
    "                       timestamp=i/test_env.SIM_FREQ,\n",
    "                       state= np.hstack([obs[0:3], np.zeros(4), obs[3:15],  np.resize(action, (4))]),\n",
    "                       control=np.zeros(12)\n",
    "                       )\n",
    "        sync(np.floor(i*test_env.AGGR_PHY_STEPS), start, test_env.TIMESTEP)\n",
    "        # if done: obs = test_env.reset() # OPTIONAL EPISODE HALT\n",
    "    test_env.close()\n",
    "    logger.save_as_csv(\"sa\") # Optional CSV save\n",
    "    logger.plot()\n",
    "\n",
    "    # with np.load(ARGS.exp+'/evaluations.npz') as data:\n",
    "    #     print(data.files)\n",
    "    #     print(data['timesteps'])\n",
    "    #     print(data['results'])\n",
    "    #     print(data['ep_lengths'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
