{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6090e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/.local/lib/python3.8/site-packages/stable_baselines3/common/cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Learning script for single agent problems.\n",
    "\n",
    "Agents are based on `stable_baselines3`'s implementation of A2C, PPO SAC, TD3, DDPG.\n",
    "\n",
    "Example\n",
    "-------\n",
    "To run the script, type in a terminal:\n",
    "\n",
    "    $ python singleagent.py --env <env> --algo <alg> --obs <ObservationType> --act <ActionType> --cpu <cpu_num>\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Use:\n",
    "\n",
    "    $ tensorboard --logdir ./results/save-<env>-<algo>-<obs>-<act>-<time-date>/tb/\n",
    "\n",
    "To check the tensorboard results at:\n",
    "\n",
    "    http://localhost:6006/\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sys import platform\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.cmd_util import make_vec_env # Module cmd_util will be renamed to env_util https://github.com/DLR-RM/stable-baselines3/pull/197\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy as a2cppoMlpPolicy\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy as a2cppoCnnPolicy\n",
    "from stable_baselines3.sac.policies import SACPolicy as sacMlpPolicy\n",
    "from stable_baselines3.sac import CnnPolicy as sacCnnPolicy\n",
    "from stable_baselines3.td3 import MlpPolicy as td3ddpgMlpPolicy\n",
    "from stable_baselines3.td3 import CnnPolicy as td3ddpgCnnPolicy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65eb6ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebad36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../../\n",
    "%pwd # look at the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b9c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr 24 2022 23:59:41\n"
     ]
    }
   ],
   "source": [
    "from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.FlyThruGateAviary import FlyThruGateAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.TuneAviary import TuneAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import ActionType, ObservationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27f34dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/learning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd experiments/learning/\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652ec4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shared_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f87630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float: Reward threshold to halt the script.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPISODE_REWARD_THRESHOLD = -0 # Upperbound: rewards are always negative, but non-zero\n",
    "\"\"\"float: Reward threshold to halt the script.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f1d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current folder:  /home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/learning\n"
     ]
    }
   ],
   "source": [
    "# current_folder = globals()['_dh'][0]\n",
    "# print(\"current folder: \", current_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4543d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(act=<ActionType.PID: 'pid'>, algo='ppo', cpu=1, env='hover', obs=<ObservationType.KIN: 'kin'>)\n",
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "[INFO] Action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n",
      "[INFO] Observation space: Box([-1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (12,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/.local/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/nicholas/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 12:13:49.012955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nicholas/Documents/UPenn/F1TENTH/F1TENTH_project_ws/devel/lib:/home/nicholas/Documents/UPenn/F1TENTH/Notes_and_UVA/UVA/Assignments/assgn1_ws/devel/lib:/home/nicholas/Documents/UPenn/F1TENTH/ngurnard_lab1_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-04-25 12:13:49.013218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/results/save-hover-ppo-kin-pid-04.25.2022_12.13.46/tb/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/.local/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=-188.46 +/- 0.32\n",
      "Episode length: 242.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 242      |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "/home/nicholas/Documents/UPenn/CIS519/final_project/CIS519-Final-Project/experiments/results/save-hover-ppo-kin-pid-04.25.2022_12.13.46\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    desired_iterations = 3000000\n",
    "\n",
    "    #### Define and parse (optional) arguments for the script ##\n",
    "    parser = argparse.ArgumentParser(description='Single agent reinforcement learning experiments script')\n",
    "    parser.add_argument('--env',        default='hover',      type=str,             choices=['takeoff', 'hover', 'flythrugate', 'tune'], help='Task (default: hover)', metavar='')\n",
    "    parser.add_argument('--algo',       default='ppo',        type=str,             choices=['a2c', 'ppo', 'sac', 'td3', 'ddpg'],        help='RL agent (default: ppo)', metavar='')\n",
    "    parser.add_argument('--obs',        default='kin',        type=ObservationType,                                                      help='Observation space (default: kin)', metavar='')\n",
    "    parser.add_argument('--act',        default='pid',        type=ActionType,                                                           help='Action space (default: one_d_rpm)', metavar='')\n",
    "    parser.add_argument('--cpu',        default='1',          type=int,                                                                  help='Number of training environments (default: 1)', metavar='')        \n",
    "    ARGS = parser.parse_args(args=[])\n",
    "    print(ARGS)\n",
    "\n",
    "    #### Save directory ########################################\n",
    "    filename = os.path.dirname(os.path.abspath(\"\"))+'/results/save-'+ARGS.env+'-'+ARGS.algo+'-'+ARGS.obs.value+'-'+ARGS.act.value+'-'+datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
    "    if not os.path.exists(filename):\n",
    "        os.makedirs(filename+'/')\n",
    "\n",
    "    #### Print out current git commit hash #####################\n",
    "    # if platform == \"linux\" or platform == \"darwin\":\n",
    "    #     git_commit = subprocess.check_output([\"git\", \"describe\", \"--tags\"]).strip()\n",
    "    #     with open(filename+'/git_commit.txt', 'w+') as f:\n",
    "    #         f.write(str(git_commit))\n",
    "\n",
    "    #### Warning ###############################################\n",
    "    if ARGS.env == 'tune' and ARGS.act != ActionType.TUN:\n",
    "        print(\"\\n\\n\\n[WARNING] TuneAviary is intended for use with ActionType.TUN\\n\\n\\n\")\n",
    "    if ARGS.act == ActionType.ONE_D_RPM or ARGS.act == ActionType.ONE_D_DYN or ARGS.act == ActionType.ONE_D_PID:\n",
    "        print(\"\\n\\n\\n[WARNING] Simplified 1D problem for debugging purposes\\n\\n\\n\")\n",
    "    #### Errors ################################################\n",
    "        if not ARGS.env in ['takeoff', 'hover']: \n",
    "            print(\"[ERROR] 1D action space is only compatible with Takeoff and HoverAviary\")\n",
    "            exit()\n",
    "    if ARGS.act == ActionType.TUN and ARGS.env != 'tune' :\n",
    "        print(\"[ERROR] ActionType.TUN is only compatible with TuneAviary\")\n",
    "        exit()\n",
    "    if ARGS.algo in ['sac', 'td3', 'ddpg'] and ARGS.cpu!=1: \n",
    "        print(\"[ERROR] The selected algorithm does not support multiple environments\")\n",
    "        exit()\n",
    "\n",
    "    #### Uncomment to debug slurm scripts ######################\n",
    "    # exit()\n",
    "\n",
    "    env_name = ARGS.env+\"-aviary-v0\"\n",
    "    sa_env_kwargs = dict(aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS, obs=ARGS.obs, act=ARGS.act)\n",
    "    # train_env = gym.make(env_name, aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS, obs=ARGS.obs, act=ARGS.act) # single environment instead of a vectorized one    \n",
    "    if env_name == \"takeoff-aviary-v0\":\n",
    "        train_env = make_vec_env(TakeoffAviary,\n",
    "                                 env_kwargs=sa_env_kwargs,\n",
    "                                 n_envs=ARGS.cpu,\n",
    "                                 seed=0\n",
    "                                 )\n",
    "    if env_name == \"hover-aviary-v0\":\n",
    "        train_env = make_vec_env(HoverAviary,\n",
    "                                 env_kwargs=sa_env_kwargs,\n",
    "                                 n_envs=ARGS.cpu,\n",
    "                                 seed=0\n",
    "                                 )\n",
    "    if env_name == \"flythrugate-aviary-v0\":\n",
    "        train_env = make_vec_env(FlyThruGateAviary,\n",
    "                                 env_kwargs=sa_env_kwargs,\n",
    "                                 n_envs=ARGS.cpu,\n",
    "                                 seed=0\n",
    "                                 )\n",
    "    if env_name == \"tune-aviary-v0\":\n",
    "        train_env = make_vec_env(TuneAviary,\n",
    "                                 env_kwargs=sa_env_kwargs,\n",
    "                                 n_envs=ARGS.cpu,\n",
    "                                 seed=0\n",
    "                                 )\n",
    "    print(\"[INFO] Action space:\", train_env.action_space)\n",
    "    print(\"[INFO] Observation space:\", train_env.observation_space)\n",
    "    # check_env(train_env, warn=True, skip_render_check=True)\n",
    "    \n",
    "    #### On-policy algorithms ##################################\n",
    "    onpolicy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                           net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])]\n",
    "                           ) # or None\n",
    "    if ARGS.algo == 'a2c':\n",
    "        model = A2C(a2cppoMlpPolicy,\n",
    "                    train_env,\n",
    "                    policy_kwargs=onpolicy_kwargs,\n",
    "                    tensorboard_log=filename+'/tb/',\n",
    "                    verbose=1\n",
    "                    ) if ARGS.obs == ObservationType.KIN else A2C(a2cppoCnnPolicy,\n",
    "                                                                  train_env,\n",
    "                                                                  policy_kwargs=onpolicy_kwargs,\n",
    "                                                                  tensorboard_log=filename+'/tb/',\n",
    "                                                                  verbose=1\n",
    "                                                                  )\n",
    "    if ARGS.algo == 'ppo':\n",
    "        model = PPO(a2cppoMlpPolicy,\n",
    "                    train_env,\n",
    "                    policy_kwargs=onpolicy_kwargs,\n",
    "                    tensorboard_log=filename+'/tb/',\n",
    "                    verbose=1\n",
    "                    ) if ARGS.obs == ObservationType.KIN else PPO(a2cppoCnnPolicy,\n",
    "                                                                  train_env,\n",
    "                                                                  policy_kwargs=onpolicy_kwargs,\n",
    "                                                                  tensorboard_log=filename+'/tb/',\n",
    "                                                                  verbose=1\n",
    "                                                                  )\n",
    "\n",
    "    #### Off-policy algorithms #################################\n",
    "    offpolicy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                            net_arch=[512, 512, 256, 128]\n",
    "                            ) # or None # or dict(net_arch=dict(qf=[256, 128, 64, 32], pi=[256, 128, 64, 32]))\n",
    "    if ARGS.algo == 'sac':\n",
    "        model = SAC(sacMlpPolicy,\n",
    "                    train_env,\n",
    "                    policy_kwargs=offpolicy_kwargs,\n",
    "                    tensorboard_log=filename+'/tb/',\n",
    "                    verbose=1\n",
    "                    ) if ARGS.obs==ObservationType.KIN else SAC(sacCnnPolicy,\n",
    "                                                                train_env,\n",
    "                                                                policy_kwargs=offpolicy_kwargs,\n",
    "                                                                tensorboard_log=filename+'/tb/',\n",
    "                                                                verbose=1\n",
    "                                                                )\n",
    "    if ARGS.algo == 'td3':\n",
    "        model = TD3(td3ddpgMlpPolicy,\n",
    "                    train_env,\n",
    "                    policy_kwargs=offpolicy_kwargs,\n",
    "                    tensorboard_log=filename+'/tb/',\n",
    "                    verbose=1\n",
    "                    ) if ARGS.obs==ObservationType.KIN else TD3(td3ddpgCnnPolicy,\n",
    "                                                                train_env,\n",
    "                                                                policy_kwargs=offpolicy_kwargs,\n",
    "                                                                tensorboard_log=filename+'/tb/',\n",
    "                                                                verbose=1\n",
    "                                                                )\n",
    "    if ARGS.algo == 'ddpg':\n",
    "        model = DDPG(td3ddpgMlpPolicy,\n",
    "                    train_env,\n",
    "                    policy_kwargs=offpolicy_kwargs,\n",
    "                    tensorboard_log=filename+'/tb/',\n",
    "                    verbose=1\n",
    "                    ) if ARGS.obs==ObservationType.KIN else DDPG(td3ddpgCnnPolicy,\n",
    "                                                                train_env,\n",
    "                                                                policy_kwargs=offpolicy_kwargs,\n",
    "                                                                tensorboard_log=filename+'/tb/',\n",
    "                                                                verbose=1\n",
    "                                                                )\n",
    "\n",
    "    #### Create eveluation environment #########################\n",
    "    if ARGS.obs == ObservationType.KIN: \n",
    "        eval_env = gym.make(env_name,\n",
    "                            aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS,\n",
    "                            obs=ARGS.obs,\n",
    "                            act=ARGS.act\n",
    "                            )\n",
    "    elif ARGS.obs == ObservationType.RGB:\n",
    "        if env_name == \"takeoff-aviary-v0\": \n",
    "            eval_env = make_vec_env(TakeoffAviary,\n",
    "                                    env_kwargs=sa_env_kwargs,\n",
    "                                    n_envs=1,\n",
    "                                    seed=0\n",
    "                                    )\n",
    "        if env_name == \"hover-aviary-v0\": \n",
    "            eval_env = make_vec_env(HoverAviary,\n",
    "                                    env_kwargs=sa_env_kwargs,\n",
    "                                    n_envs=1,\n",
    "                                    seed=0\n",
    "                                    )\n",
    "        if env_name == \"flythrugate-aviary-v0\": \n",
    "            eval_env = make_vec_env(FlyThruGateAviary,\n",
    "                                    env_kwargs=sa_env_kwargs,\n",
    "                                    n_envs=1,\n",
    "                                    seed=0\n",
    "                                    )\n",
    "        if env_name == \"tune-aviary-v0\": \n",
    "            eval_env = make_vec_env(TuneAviary,\n",
    "                                    env_kwargs=sa_env_kwargs,\n",
    "                                    n_envs=1,\n",
    "                                    seed=0\n",
    "                                    )\n",
    "        eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "    #### Train the model #######################################\n",
    "    # checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=filename+'-logs/', name_prefix='rl_model')\n",
    "    callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=EPISODE_REWARD_THRESHOLD,\n",
    "                                                     verbose=1\n",
    "                                                     )\n",
    "    eval_callback = EvalCallback(eval_env,\n",
    "                                 callback_on_new_best=callback_on_best,\n",
    "                                 verbose=1,\n",
    "                                 best_model_save_path=filename+'/',\n",
    "                                 log_path=filename+'/',\n",
    "                                 eval_freq=int(2000/ARGS.cpu),\n",
    "                                 deterministic=True,\n",
    "                                 render=False\n",
    "                                 )\n",
    "    model.learn(total_timesteps=desired_iterations, #int(1e12),\n",
    "                callback=eval_callback,\n",
    "                log_interval=100,\n",
    "                )\n",
    "\n",
    "    #### Save the model ########################################\n",
    "    model.save(filename+'/success_model.zip')\n",
    "    print(filename)\n",
    "\n",
    "    #### Print training progression ############################\n",
    "    # with np.load(filename+'/evaluations.npz') as data:\n",
    "    #     for j in range(data['timesteps'].shape[0]):\n",
    "    #         print(str(data['timesteps'][j])+\",\"+str(data['results'][j][0][0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
